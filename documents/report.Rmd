---
title: "ML2 Project"
author: "Eric Jonas, Caroline Graebel, Naim Iskandar Zahari, Kseniia Holovchenko"
date: "`r Sys.Date()`"
output: 
  bookdown::gitbook:
    css: "custom.css"
    code_folding: hide
    split_by: none
    self_contained: true
  # if pdf wanted
  #pdf_document:
  #  latex_engine: xelatex
bibliography: references.bib
csl: norsk-apa-manual.csl
nocite:
- "@corriebar_apartment_rental_germany_2020"
- "@rasifaghihi_support_vector_regressor_2023"
- "@gunay_understanding_parameters_svm_2021"
- "@lejlot_svc_coef0_parameter_range_2013"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 1, digits = 2)
source("./code/setup.R") # Knit Directory = project dir (document dir is default set manually to project dir)

full_refit = F
output_type = "gitbook" # "gitbook" or "pdf"
```

# Introduction

Understanding rent prices in Berlin is essential for anyone looking to rent an apartment in the city, given its dynamic housing market and the growing demand for affordable living spaces. To address this, our project focuses on analyzing historical rental data from Berlin in 2020, aiming to uncover insights and develop predictive models that estimate rental prices based on key property attributes and evaluate them on current apartment offers.

The following dataset, available at [Kaggle](https://www.kaggle.com/datasets/corrieaar/apartment-rental-offers-in-germany/data), contains information about rental properties listed on ImmoScout24. The data was scraped from ImmoScout24 between February and October 2020 and includes listings from all German federal states. For our project, we will focus exclusively on Berlin and train a model to predict either the `totalRent` or the `basePrice`.

## Data Description

The data provided in the Kaggle dataset consists of approximately 268,000 rental listings in Germany. In the dataset, the information from the rental listings are structured in a CSV file with 49 columns. The columns include:

- Geographical information (state, city name and zip code)
- Property information (construction year, flat type, heating type and number of rooms)
- Total rent, base rent and service charge (in Euros)

## Objectives

- **Data Understanding and Cleaning**:\
  Before training a model, it is essential to thoroughly understand and clean the data. To achieve this, we will provide a concise visual overview of the dataset to highlight its key characteristics and identify potential issues.

- **Predict Missing Values**:\
  Because many columns in our data contain missing values, one of our goals is to impute these values and use the resulting data instead of the raw data.

- **Model Selection and Comparison**:\
  We aim to train a series of tree-based models on the dataset to predict the `baseRent` of an apartment, starting from simple decision trees that can provide explainable predictions and gradually progressing to more complex models such as Random Forests. Additionally, we will compare the performance of these tree-based methods with Support Vector Machine (SVM) regression to determine which approach yields the best results.

# Data Overview and Cleaning

Historical rental listing data for Berlin is crucial for achieving the objectives of this project. The ImmoScout24 dataset provides rental listings from all of Germany. To begin, we need to extract data specific to Berlin from the full dataset. This extracted data contains numerous missing values and redundant entries, making it essential to first thoroughly understand and clean the dataset before proceeding.

## Data Preparation

After filtering for Berlin listings, more than 10,000 entries remain. The next step is to remove irrelevant fields from the dataset, which will help with the development of machine learning models in subsequent stages. Identifying and removing these irrelevant fields requires a thorough understanding of the dataset. A brief explanation of each column can be found on Kaggle. Below is a list of the fields removed, along with the reasons for their exclusion:

| Column Name                | Reason for Removal                                                                                   |
|----------------------------|------------------------------------------------------------------------------------------------------|
| regio1                     | Redundant, as it contains only "Berlin."                                                             |
| geo_bln                    | Redundant, as it contains only "Berlin."                                                             |
| geo_krs                    | Redundant, as it contains only "Berlin."                                                             |
| regio2                     | Redundant, as it contains only "Berlin."                                                             |
| street                     | Redundant information; contains the same data as `street_plain`, but of lower quality.               |
| telekomTvOffer             | Irrelevant, as all listings can offer this service.                                                  |
| telekomHybridUploadSpeed   | Unnecessary, with mostly missing (NA) values.                                                        |
| scoutId                    | Unique ID, not useful for analysis.                                                                  |
| yearConstructedRange       | Artificial column, not relevant to the analysis.                                                     |
| streetPlain                | Would require significant cleaning or conversion into longitude and latitude.                        |
| houseNumber                | Requires a corresponding street to provide meaningful context.                                       |
| baseRentRange              | Artificial column, not needed for analysis.                                                          |
| livingSpaceRange           | Artificial column, not needed for analysis.                                                          |
| noRoomsRange               | Artificial column, not needed for analysis.                                                          |
| electricityKwhPrice        | Deprecated since 2020; irrelevant as the dataset lacks earlier data.                                 |
| electricityBasePrice       | Deprecated since 2020; irrelevant.                                                                   |
| energyEfficiencyClass      | Deprecated since 2020; irrelevant.                                                                   |
| date                       | Redundant, as it duplicates `date_full`.                                                             |
| description                | Redundant, as it only repeats information already provided in other columns.                         |
| facilities                 | Redundant, as it only repeats information already provided in other columns.                         |
| totalRent                  | Artificial column, a sum of baseRent, heatingCosts, and serviceCharge; unnecessary for analysis.     |
| newlyConst                 | Artificial column, constructionYear >= 2017 ? TRUE : FALSE                                           |

<br>
Upon removing the irrelevant fields from the Berlin dataset, the remaining fields include:

```{r data_overview, echo=FALSE, cache=!full_refit}
# Display a detailed summary of the dataset
if(output_type == "gitbook") {
  print(
    summarytools::dfSummary(
      varnumbers   = FALSE,
      na.col       = FALSE,
      berlin_data_cleaned, 
      plain.ascii = FALSE, 
      style = "grid",
      graph.magnif = .5
      ), 
    method = 'render')
} else {
  str(berlin_data_cleaned)
}
```



## Data Analysis

```{r data_analysis_setup, echo=FALSE, cache=!full_refit}
berlin_data_cleaned_visualizations <- berlin_data_cleaned
berlin_data_cleaned_visualizations$district_label <- paste(berlin_data_cleaned_visualizations$DISTRICT_ID, 
                                                           berlin_data_cleaned_visualizations$district, sep = " - ")

berlin_data_cleaned_visualizations$log_baseRent <- log(berlin_data_cleaned_visualizations$baseRent)
  
berlin_data_district_statistics <- berlin_data_cleaned_visualizations %>%
  filter(!is.na(district)) %>%
  group_by(district, district_label) %>% 
  summarize(count_apartments = n(),
            median_base_rent = median(baseRent, na.rm = TRUE),
            , .groups = 'drop') %>%
  left_join(berlin_district_geo, by = c("district" = "DISTRICT_NAME")) %>%
  filter(!is.na(district))
```

To understand the contents of the columns in our dataset and how they interact, we visualize them. We will focus on how the districts are distributed across our dataset and how attributes, such as the `baseRent`, are distributed within them. Additionally, we will examine the apartments in our dataset and investigate whether there are any correlations present.


### District-Level Apartment Overview and Model Considerations

As seen in the data overview chapter, our dataset does not include the districts directly but only the postal codes (`geo_plz`). To derive the districts, we join our dataset with a reference dataset that maps Berlin's postal codes to districts. It is important to note that some postal codes belong to multiple districts. For simplicity, we associate each postal code with only one district. This simplification introduces a bias towards districts which appear earlier in our data, The following figure displays the frequency of each district represented in our dataset.

```{r dataAnalysisDistrictCount, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap= "Apartment Count by District in Berlin", fig.width=8, fig.height=4.5}
districts_count_map <- ggplot() +
  geom_sf(data = berlin_data_district_statistics, aes(geometry = geometry, fill = count_apartments)) +
  geom_sf_text(data = berlin_data_district_statistics, aes(geometry = geometry, label = DISTRICT_ID), size = 3, color = "black") +
  scale_fill_gradient(low = "lightblue", high = "dodgerblue4", name = "Apartment Count") +
  theme_void() +
  theme(legend.position = "right", plot.title = element_text(hjust = 0.5)) +
  guides(fill = "none")

districts_count_barchart <- ggplot(berlin_data_district_statistics, 
                                   aes(x = reorder(district_label, count_apartments), y = count_apartments, fill = count_apartments)) +
  geom_col() +
  scale_fill_gradient(low = "lightblue", high = "dodgerblue4", name = "Apartment Count") +
  labs(x = "District", y = "Apartment Count") +
  theme_minimal() +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        plot.title = element_text(hjust = 0.5)) +
  guides(fill = "none")

grid.arrange(districts_count_map, districts_count_barchart, ncol=2, widths = c(1.5, 2))
```

We observe that districts in central and eastern parts of Berlin are more frequently represented in the dataset compared to those in the southern or peripheral areas. Based on this we will evaluate later whether our model is bias towards districts that appear more frequently.

Furthermore, we analyzed the median prices across different districts. Our initial assumption was that districts closer to the city center have a higher median price compared to those in the outskirts. This assumption is based on the knowledge  that central areas typically offer superior infrastructure and greater access to amenities and activity hubs.

```{r dataAnalysisDistrictRent, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE, fig.align='center', fig.width=8, fig.height=4.5, fig.cap= "Median Price by District in Berlin"}
districts_median_rent_map <- ggplot() +
  geom_sf(data = berlin_data_district_statistics, aes(geometry = geometry, fill = median_base_rent)) +
  geom_sf_text(data = berlin_data_district_statistics, aes(geometry = geometry, label = DISTRICT_ID), size = 3, color = "black") +
  scale_fill_gradient(low = "lightyellow", high = "darkorange", name = "Median Rent") +
  theme_void() +
  theme(legend.position = "right", plot.title = element_text(hjust = 0.5)) +
  guides(fill = "none")

berlin_data_cleaned_log <- berlin_data_cleaned_visualizations %>%
  filter(!is.na(district) & !is.na(log_baseRent) & is.finite(log_baseRent)) %>%
  left_join(berlin_data_district_statistics, by = "district_label")

districts_rent_boxplot <- ggplot(berlin_data_cleaned_log, aes(x = reorder(district_label, log_baseRent, FUN = median), 
                                    y = log_baseRent, fill = median_base_rent)) +
  geom_boxplot() +
  scale_fill_gradient(low = "lightyellow", high = "darkorange", name = "Median Rent") +
  labs(x = "District", y = "Log Base Rent") +
  theme_minimal() +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1.5), 
        plot.title = element_text(hjust = 0.5)) +
  guides(fill = "none")

grid.arrange(districts_median_rent_map, districts_rent_boxplot, ncol=2, widths = c(1.5, 2))
```

Our initial assumption is confirmed: districts closer to the city center generally have higher rental prices, with Mitte standing out with several significant outliers. These outliers could pose challenges for our model if it lacks the robustness to accommodate them. Interestingly, when comparing this plot with the earlier analysis of apartment counts per district, the correlation between rental prices and the number of apartments appears weak. For example, Treptow-Köpenick, despite having a high number of apartments, shows relatively low rental prices.


### Analysis of Rental Characteristics

The next step in understanding our dataset is to analyze characteristics of the apartments. This gives us an initial insight into the typical apartment types in our data and helps identify anomalies. First, we examine the distribution of the number of rooms across the apartments.

```{r dataAnalysisRooms, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap= "Count of Apartments by Number of Rooms", fig.width=6, fig.height=4}
berlin_data_cleaned_visualizations$noRoomsRounded <- round(berlin_data_cleaned_visualizations$noRooms)

ggplot(berlin_data_cleaned_visualizations, aes(x = factor(noRoomsRounded))) +
  geom_bar(fill = "steelblue", alpha = 0.7) +
  labs(x = "Number of Rooms",
       y = "Count") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

From the distribution, it is evident that the vast majority of apartments have a reasonable number of rooms, ranging between 1 and 8. However, there is one significant outlier, an apartment with an astonishing 100 rooms. While most of the data falls within expected ranges, such an outlier could have a disproportionate influence on our analysis and modeling.

One of our hypotheses about this dataset is that there should be a strong positive correlation between the number of rooms and the living space. Intuitively, as the number of rooms increases, the total living space is also expected to increase. Similarly, for smaller apartments, fewer rooms should correspond to smaller living spaces.  
To visualize the relationship between living space and the number of rooms, we decided to exclude room counts that appear fewer than 10 times in the following visualization. This threshold ensures that the data we analyze is statistically meaningful. As a result, we focus on apartments with room counts greater than 8 for the following boxplot.

```{r dataAnalysisRoomsLivingspace, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap= "Living Space Distribution by Number of Rooms", fig.width=6, fig.height=4}
berlin_data_room_no_outliers <- berlin_data_cleaned_visualizations %>%
  filter(noRoomsRounded <= 8)

# Boxplot
ggplot(berlin_data_room_no_outliers, aes(x = factor(noRoomsRounded), y = livingSpace)) +
  geom_boxplot(aes(fill = factor(noRoomsRounded)), alpha = 0.7) +
  labs(x = "Number of Rooms", 
       y = "Living Space (Square Meters)") +
  scale_fill_brewer(palette = "Blues") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  guides(fill = "none")
```

The filtered boxplot confirms this hypothesis. Even for a room count like 8, which only has 16 occurrences, the trend remains consistent. Larger apartments have larger living spaces, which aligns with our expectations.  
A noteworthy observation is the prevalence of outliers in apartments with up to 5 rooms, especially for those with 3 and 4 rooms. These outliers are highly dispersed, potentially indicating that some of these entries might represent office spaces rather than standard apartments.

Another important aspect of our dataset is the condition of the apartments. This feature provides qualitative information about the state of the properties, with categories like *mint_condition*, *refurbished*, and *renovated*. However, the definitions of these categories are not immediately clear, particularly the distinction between "mint_condition" and the other classifications.  
To better understand these categories, we compare the condition of the apartments to their rent values. This comparison allows us to infer potential meanings or differences between the conditions.

```{r dataAnalysisConditions, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap= "Base Rent by Conditions", fig.width=6, fig.height=4}
ggplot(berlin_data_cleaned_visualizations, aes(x = reorder(condition, baseRent, FUN = median), y = baseRent, fill = condition)) +
  geom_boxplot() +
  scale_fill_brewer(palette = "Blues") +
  scale_y_log10() +
  coord_flip() +
  labs(title = "Base Rent by Condition", x = "Condition", y = "Base Rent (EUR)") +
  theme_minimal() +
  theme(axis.text.x = element_text(size = 10), axis.text.y = element_text(size = 10)) +
  guides(fill = "none")
```

A clear order emerges among the apartment conditions. Apartments classified as *mint_condition* appear to represent the best quality. Similarly, *refurbished* properties seem to have a better condition than *renovated*. This ordering aligns with the intuition that better maintained or new  apartments tend to have higher rents. Outliers might represent unique cases, such as misclassifications, unusual rental agreements or exceptional property features.  
Another observation is that the missing entries (`NA`) have a wide interquartile range and contain several outliers, which indicates that these entries are spread across categories and may not represent a single uniform condition. This variability could introduce noise into our analysis and modeling. To address this, we aim to impute the missing condition values.


### Exploring Correlations Between Variables

To better understand the relationships between variables in our dataset, we analyzed the correlations between them, focusing on how the predictors relate to the `baseRent`. This helps us identify expected patterns and uncover any unexpected relationships.

```{r dataAnalysisCorrelations, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap= "Correlation Matrix", fig.width=7, fig.height=7}
numeric_vars <- berlin_data_cleaned_visualizations %>% select_if(is.numeric) %>% select(-log_baseRent, -noRoomsRounded)
cor_matrix <- cor(numeric_vars,  use = "complete.obs")
corrplot::corrplot(cor_matrix, method = "circle", type = "lower", 
                   tl.cex = 0.8,
                   mar = c(0, 0, 1, 0))
```

From the correlation analysis, we observed several expected relationships:

- Apartments with a high `baseRent` tend to have:
  - Higher `serviceCharges`, `heatingCosts` (additional rental costs)
  - Larger `livingSpace`, more rooms (`noRooms`)

These findings align with our expectations, as more expensive apartments generally offer more space and additional services.

One initially confusing correlation was between `thermalChar` and `yearConstructed`. After further research, we found that `thermalChar` represents the apartments energy efficiency level, which is often linked to the buildings construction year. Newer buildings tend to be more energy-efficient due to modern construction standards and regulations.  
Interestingly, this energy efficiency only shows a weak negative correlation with `heatingCosts`. Contrary to our inital expectations, more energy-efficient apartments appear to incur slightly higher heating costs. One possible explanation is the cost disparity between energy sources, in 2020 and even today, dirtier energy sources are often more affordable than renewable or sustainable alternatives.


## Imputation

Missing values in the data can significantly impact model performance, especially for Support Vector Machines (SVMs). While tree models such as those in `rpart` can handle missing values, SVMs from the `e1071` package cannot. SVMs simply exclude rows with missing values, which can lead to a scenario where the model is trained on as few as 10 observations.

In our initial [data overview](#data-preparation) we identified the number of missing values in each column. To improve our understanding, we will visualize the top 10 columns with the highest percentage of missing values. These columns are also potential candidates for exclusion if we decide to reduce our model size. The following plot displays the absolute number of missing values out of a total of 10,304 entries.

```{r dataAnalysisMissingValues, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap= "Top 10 Columns with Most Missing Values", fig.width=6, fig.height=4}
top_na_counts <- berlin_data_cleaned %>%
  summarise(across(everything(), ~ sum(is.na(.)), .names = "{col}")) %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "MissingCount") %>%
  arrange(desc(MissingCount)) %>%
  slice_head(n = 10)

ggplot(top_na_counts, aes(x = reorder(Column, MissingCount), y = MissingCount)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Columns", y = "Count of Missing Values") +
  theme_minimal() +
  coord_flip()
```

For some columns, such as `lastRefurbished`, missing values have specific meanings. For example, indicating that the apartment has never been renovated. For the remaining columns we assume that the missing values represent missing data.   
To address the values in `lastRefurbished` we will impute missing values with the year of construction, as this represents the most plausible time for the first or last renovation.  
For all other columns we will use *Gibbs sampling* to impute values. This approach preserves the underlying distribution of variables and minimizes information loss during model fitting. 

The imputation process using the *MICE* algorithm will generate five different datasets. For model training we will build models on all five datasets and aggregate their results to produce a final output. For simplicity in visualizations, such as decision tree plots, we will use the first dataset generated by Gibbs sampling.



# Model Overview

The following chapter begins with an overview of the machine learning algorithms we plan to use for our models. The content in this section is primarily based on our lecture slides, with citations provided for any external sources. Subsequently, we will train various models and evaluate them.

## Regression Trees

Tree models are supervised learning algorithms used for both regression and classification. They work by splitting the dataset into smaller subsets based on decision rules derived from the features and forming a tree structure. Each split improves the prediction accuracy of the model on the training data. The process continues until a stopping criterion, such as minimum number of observations in a node or maximum tree depth, is met.
The tree’s leaf nodes represent the final predictions. For regression trees, this prediction is typically the average of the target variable within that leaf.  
Regression trees are particularly useful for predicting continuous values as they can easily capture non-linear relationships. In our case, we aim to predict `baseRent` using a regression tree model.

The following procedure shows how regression trees work:

1. **Initialization**:  
  The process begins with the entire dataset. The mean of the target variable \( Y \) serves as an initial prediction, as it minimizes the **Residual sum of squares (RSS)**:
  \[
  RSS = \sum_{i=1}^n (y_i - \hat{y})^2,
  \]
  where \( \hat{y} \) is the mean of \( Y \).

2. **Splitting the data**:  
  At each node, the dataset is split into two regions \( R_1 \) and \( R_2 \), based on feature \( X_j \) and threshold \( x \):
  \[
  R_1: X_j < x, \quad R_2: X_j \geq x.
  \]
  The goal is to find the split that minimizes the RSS for the two regions:
  \[
  RSS = \sum_{i \in R_1} (y_i - \hat{y}_1)^2 + \sum_{i \in R_2} (y_i - \hat{y}_2)^2,
  \]
  where \( \hat{y}_1 \) and \( \hat{y}_2 \) are the mean values of \( Y \) in \( R_1 \) and \( R_2 \), respectively.

3. **Recursive splitting**:  
  This process of splitting is repeated recursively. At every step, the algorithm evaluates all possible splits for every feature and selects the one that minimizes the RSS.

4. **Stopping criteria**:  
  If we continue splitting until each node contained only a single observation, then each node would perfectly predict the value of its observation, leading to an overfitted model with an RSS of 0.  
  That is why we stop the tree growth when a predefined stopping criterion is met. This could be:
    - The minimum number of observations in a node is reached.  
    - The maximum depth is reached.  
    - Further splits fail to reduce the RSS significantly.

5. **Pruning with penalized least squares**:  
  Overfitting is a common problem with regression trees when they grow too large. To address this, the tree is pruned using **Penalized least squares (PLS)**:
  \[
  PLS(\alpha) = \sum_{j=1}^{|T_\alpha|} \sum_{i \in R_j} (y_i - \hat{y}_j)^2 + \alpha |T_\alpha|,
  \]
  where:
    - \( |T_\alpha| \) is the number of terminal nodes (leaf nodes) in the pruned tree \( T_\alpha \),
    - \( \alpha \geq 0 \) is a complexity parameter controlling the trade-off between tree size and prediction accuracy.
  
    The parameter \( \alpha \) is chosen by using cross-validation, which ensures the pruned tree balances model complexity and performance.

6. **Choosing the best subtree**:  
  To find the optimal tree:
    - The full tree is grown.
    - The pruning process evaluates subtrees by varying \( \alpha \) and calculates the relative error using cross-validation.
    - The subtree corresponding to the smallest cross-validation error is selected.


## Ensemble Methods

To understand how tree ensemble methods work, we need to know what bootstrapping is.
Bootstrapping is a statistical technique that involves sampling with replacement from a dataset to create multiple subsets. Each bootstrap sample is the same size as the original dataset, but some observations may appear multiple times while others may be excluded. This technique is commonly used to generate diverse training subsets or to augment small datasets. In ensemble methods like bagging and random forests, bootstrapping enhances generalization and robustness by reducing variance only slightly increasing bias.


### Bagging

Bagging is an ensemble learning technique that enhances accuracy and reduces variance by combining the predictions of multiple models. It involves training several instances of the same machine learning algorithm on different subsets of the data and then aggregating their predictions to form a final output. For regression tasks, this typically involves averaging the predictions. For classification tasks, the class with the majority vote is selected.

Bagging reduces model variance by averaging out individual errors, which leads to more robust and reliable predictions. This technique is especially useful when using base models that are prone to high variance, such as decision trees. An ensemble of decision trees is commonly referred to as a *forest*.


### Random Forests

*Random forests* enhance the concept of *forests* by introducing additional randomness during the tree building process. At each node split, only a random subset of features is considered, which prevents the model from relying too heavily on the same dominant predictors. This also allows less important predictors to capture more subtle patterns in the data. Typically, the number of features \( m \) considered at each split is given by \( m = \lfloor \sqrt{p} \rfloor \), where \( p \) is the total number of features. This approach helps to reduce overfitting caused by a few strong predictors, which leads to more robust and reliable predictions.


### Boosting

Boosting is an ensemble learning technique in machine learning that aims to improve the performance of weak models, such as a small tree (called *stump*) by combining their predictions to create a strong predictive model. The core idea is to iteratively train a series of models, where each new model focuses on correcting the mistakes made by the previous ones.

The process begins with a simple model, like a stump, which makes predictions on the training data. The instances that were misclassified or had large errors are then given higher weights, so that subsequent models pay more attention to correcting these errors.  
In the next iteration, a new model is trained on the weighted data, where the misclassified have higher weights. After training, it is added to the ensemble and its predictions are combined with those of the previous models. The respect to errors the weights are updated again. This process repeats for several iterations, with each new model gradually improving the overall predictions.  
The final prediction is made by aggregating the predictions of all models. In algorithms like AdaBoost, this is done by weighted voting, where better performing models have more influence on the result.

Boosting helps to reduce bias and variance. Initially, a simple model may have high bias and low variance, but as boosting progresses, the bias decreases, which improves the accuracy of our predictions. However, the increased flexibility and strong focus on errors can also lead to higher variance, making the model more sensitive to noise. To prevent overfitting regularization techniques and early stopping are often used.

We want to test the following boosting algorithms on our dataset:

- **Gradient Boosting**: Focuses on minimizing a loss function by fitting new models to the residuals of previous predictions.
- **XGBoost**: An optimized version of gradient boosting that enhances speed, performance and uses regularization techniques.


## Support Vector Machines

Support vector machines (SVMs) are supervised learning algorithms that can be used for both classification and regression tasks. They work by finding a hyperplane that best separates data points into different classes. SVMs excel in situations where relationships between variables are complex and potentially non-linear. This chapter provides an overview of how SVMs work, with a focus on their extension to regression tasks and the kernel trick, which enables handling non-linear relationships effectively.


### Linear Support Vector Machines

In classification SVMs aim to identify a hyperplane \( f(x) \) that satisfies:  
\[
\hat{y}(x) = \text{sign}(f(x)), \quad f(x) = \beta_0 + \sum_{j=1}^p \beta_j x_j,
\]
where the hyperplane separates the data into distinct classes while maximizing a margin, which is the distance between the hyperplane and the closest data points from each class, known as *support vectors*.  

However, real world datasets often contain overlapping classes, making perfect separation impossible. To address this SVMs use a *soft margin* that allows some misclassifications. The trade off between margin width and classification errors is controlled by the penalty parameter \( C \). The optimization problem for a soft margin SVM is:  
\[
\text{maximize } M, \quad \text{subject to } y_i f(x_i) \geq M(1 - \xi_i), \quad \xi_i \geq 0, \quad \sum_{i=1}^n \xi_i \leq C,
\]
where \( \xi_i \) are slack variables representing the extent of misclassification.  

While linear SVMs are effective for linearly separable data, they struggle with non-linear relationships, where a simple hyperplane cannot adequately separate the classes.


### Non-linear Support Vector Machines

To overcome the limitations of linear SVMs, data can be projected into higher-dimensional feature spaces, where a linear hyperplane can separate the data. This involves mapping the input \( x \) from its original space \( \mathbb{R}^p \) to a higher-dimensional feature space \( \mathbb{R}^q \), where \( q > p \):  
\[
\phi(x) : \mathbb{R}^p \to \mathbb{R}^q.
\]
In this transformed space, a linear decision boundary may successfully classify the data. However, explicitly computing \( \phi(x) \) is computationally inefficient, especially for large datasets or high-dimensional mappings.  

The *kernel trick* addresses this challenge by avoiding explicit computation of \( \phi(x) \). Instead, SVMs calculate the inner product in the feature space \( \langle \phi(x_i), \phi(x_j) \rangle \), indirectly using a kernel function \( K(x_i, x_j) \). This enables efficient computation in the higher-dimensional space without ever explicitly performing the transformation. The decision function then takes the form:  
\[
f(x) = \beta_0 + \sum_{i \in S} \alpha_i K(x, x_i),
\]
where \( S \) represents the set of support vectors that define the hyperplane, enhancing computational efficiency by focusing only on these critical points. The values \( \alpha_i \) are the Lagrange multipliers corresponding to each support vector \( x_i \). These multipliers are determined during the training process and reflect the importance of each support vector in defining the optimal hyperplane.  
By leveraging the kernel trick, SVMs maintain computational efficiency while gaining the flexibility to tackle complex, non-linear problems.  


### Regression with Support Vector Machines [@rasifaghihi_support_vector_regressor_2023]

Support vector regression (SVR) is an extension of SVMs for regression tasks. The goal of SVR is to identify a function \( f(x) \) that accurately predicts the target variable, while minimizing model complexity to enhance generalization and reduce overfitting.  

A distinctive feature of SVR is the introduction of an \( \epsilon \)-insensitive tube around the regression hyperplane. This tube defines a tolerance margin within which deviations between predicted and actual target values are not penalized. This creates a balance between model complexity and its ability to generalize effectively.  

For a linear function \( f(x) = wx + b \), this means to minimizing \( w \), which makes it less sensitive to small variations in the input data.  
The SVR problem is formulated as a convex optimization task with the following objectives:  
1. Ensure that the deviations between predictions \( f(x) \) and actual target values \( y \) remain within \( \epsilon \) for all training data points. 
2. Minimize the complexity of the function \( f(x) \) by keeping \( w \) small.  

In cases where the optimization problem is infeasible due to noisy data or overlapping regions, *slack variables* (\( \xi_i^+ \) and \( \xi_i^- \)) are introduced. These variables allow for deviations beyond the \( \epsilon \)-insensitive tube, representing data points that fall outside the tube.  

The optimization problem can be expressed as:  
\[
\min \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i^+ + \xi_i^-),
\]
subject to the following constraints:  
\[
y_i - f(x_i) \leq \epsilon + \xi_i^+, \quad f(x_i) - y_i \leq \epsilon + \xi_i^-, \quad \xi_i^+, \xi_i^- \geq 0.
\]
The regularization strength is inversely related to \( C \), meaning higher values allow the model to prioritize accuracy at the expense of simplicity.  

SVR assigns zero prediction error to points within the \( \epsilon \)-insensitive tube. For points outside the tube (slack variables), the penalty is proportional to the magnitude of their deviation. The tolerance for small deviations and penalties for larger errors make SVR more robust to overfitting.


# Model Implementation
This chapter describes the implementation of the models trained on our dataset, outlining key considerations and decisions made to optimize our models. The following topics are covered: model training, hyperparameter optimization, managing outliers and reducing model size.

One challenge we faced was the diversity of geographic data inside the `geo_plz` variable, which includes 214 unique levels across 10,000 rows. This introduces the potential issue of encountering postal codes in the test data that were not present in the training data. To address this we removed the `geo_plz` variable by grouping data into broader districts and subdistricts within the `regio3` category. This also helps to capture meaningful geographic relationships while minimizing overfitting risks.

The implementation of all basic models is included in `eric_model.R`. Hyperparameter optimization (HPO) is conducted exclusively on a single Gibbs-imputed dataset in `eric_hpo.R`. This approach was selected to save computational resources, as Gibbs iterations maintain consistent distributions and variables with the highest correlations in our dataset have fewer missing values.

The implementation of all basic models is contained in the `eric_model.R`. Hyperparameter optimization (HPO) is performed exclusively on one Gibbs imputed dataset. This decision was made to save computation power and because Gibbs iterations retain consistent distributions, which means that variance captured could stays the same.

All models are then trained using the hyperparameters found through HPO on all of our imputed datasets to evaluate its performance. Additionally, outlier management is an aspect of our analysis, with experiments conducted both including and excluding outliers. Outlier free datasets are reserved for a final comparison. Finally, we will explore if it is possible to reduce the model size and evaluate how the models perform with fewer columns and potentially less noise.

```{r createGibbsTrainTest, echo=TRUE, message=FALSE, warning=FALSE, cache=!full_refit}
train_data1 <- train_data[[1]]
test_data1 <- test_data[[1]]
```


## Regression Trees

In this section, we focus on training a **regression tree** using the `rpart` package and visualizing the results. 
To determine the optimal depth or number of splits for our tree, we use cross-validation. The `rpart` package simplifies this process by calculating the cross-validated error for each of the subtrees. These cross-validation results are stored in a complexity parameter table (`cptable`), which provides the cross-validation errors for the subtrees. This table helps us to assess the complexity needed when we prune our tree. Below, we display the first 10 rows of the `cptable` for our tree:

```{r cpTable, echo=TRUE, message=FALSE, warning=FALSE, cache=!full_refit}
default_tree <- rpart(
  baseRent ~ .,
  data = train_data1,
  method = "anova"
)
print(head(default_tree$cptable, 10))
```

From the `cptable`, we can use two approaches to prune the tree:  
1. **Lowest Cross-Validation Error**: Prune the tree to the complexity parameter corresponding to the lowest cross-validated error (`xerror`).  
2. **Standard Deviation Rule**: Use the complexity parameter with the simplest tree whose error is within one standard deviation of the minimum cross-validated error. This approach is recommended by the authors of the `rpart` package.

The following plot illustrates the first complexity parameters and cross-validation errors for our tree:

```{r cpPlotDefaultTree, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap="Default Tree Complexity", fig.width=6, fig.height=4}
plotcp(default_tree) 
```

Using these methods, we pruned the full tree. The first splits of the pruned tree are shown below:

```{r prunedTree, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap="Tree First Splits", fig.width=6, fig.height=4}
pruned_tree <- prune(default_tree, cp = default_tree$cptable[6])
rpart.plot(pruned_tree) 
```

The pruning process helps to create a more interpretable tree while maintaining or even enhancing its predictive performance. Below, we compare the performance of the full tree and the two pruning techniques on the test data, averaged across all five imputed datasets. Note that the *full tree* does not necessarily imply only one node per leaf, other stopping parameters influence the structure. Having one terminal node per observation would be excessive and unnecessary.

| Method                    | Average MSE  | Average RMSE | Average MAE  |
|---------------------------|--------------|--------------|--------------|
| **Full Tree**             | 130,515.9    | 361.22       | 204.53       |
| **Best xError**           | 129,591.7    | 359.91       | 207.78       |
| **Best (Authors' Method)**| 165,022.5    | 405.1        | 253.89       |


From the table we observe the following:  
1. **Full Tree**: Surprisingly, the full tree performs well on the test dataset, achieving competitive results in MSE and RMSE and the best results for mean absolute error. This suggests a high degree of variance in the dataset.  
2. **Best xError**: This pruning technique achieves significantly better results compared to the method recommended by the authors of the `rpart` package, as it has the lowest MSE and RMSE out of all methods.  
3. **Authors Method**: While this approach prioritizes simplicity and generalization, it results in higher error values compared to the other methods. This means that this pruning method may not be the optimal choice for this dataset.



## Support Vector Machines
In this section, we focus on training a **SVM** for regression using the `e1071` package. 
SVM Regression is a distance based algorithm, which means they calculate distances between data points to make decisions. One key consideration when using SVMs is that variables with larger scales can dominate the distance calculations, potentially leading to inaccurate results. This is because the SVMs decision boundary relies on calculating the distance between points. Therefore normalizing or standardizing data is crucial to ensure that each feature contributes equally to the model. 

For tuning the SVM parameters and validating its performance, we performed cross-validation on one of our imputed datasets with the following settings, where the best parameter combination is indicated in bold.

- **Kernel Function**: (**`polynomial`**, `radial`, `sigmoid`, `linear`)  
   The kernel function defines the type of decision boundary the SVM will create. It maps the input features to a higher-dimensional space where linear separation is possible.

- **Cost (C)**: (0.5, **1**, 2, 4)  
   The cost parameter controls the trade-off between minimizing training error and ensuring good generalization to new data. It penalizes misclassification of data points. A higher cost leads to a more complex model that fits the training data closely, which may result in overfitting.

- **Gamma (γ)**: (0.01, **0.03**, 0.5, 1, 2)  
   Gamma influences individual training points when using non-linear kernels (e.g., *radial*, *polynomial*). A high gamma value causes a potentially overfitted model. A low gamma value results in a smoother decision boundary that is less sensitive to individual data points. [@gunay_understanding_parameters_svm_2021]

- **Degree**: (2, **3**, 6, 10)  
   The degree parameter applies only to the *polynomial* kernel. It specifies the degree of the polynomial used to compute the kernel. A higher degree increases the complexity which could lead to overfitting. 

- **Coef0**: (0, 1, **5**, 10)  
   The coef0 parameter is relevant only for *polynomial* and *sigmoid* kernels. It controls the impact of the higher-dimensional feature space on the decision boundary. [@lejlot_svc_coef0_parameter_range_2013]

After d
In this case the results of SVM fitting over all imputed datasets are as follows:

| Method                    | Average MSE  | Average RMSE | Average MAE  |
|---------------------------|--------------|--------------|--------------|
| **SVM**                   | 114,781.2    | 338.71       | 169.77       |

When comparing SVMs to simple regression trees, we can see that SVMs performed slightly better. Decision trees are more interpretable, but also prone to overfitting, especially when there are many features or complex relationships in the data. SVMs tend to generalize better, which leads to lower error metrics.  
While both models have their advantages, the SVMs performance suggests it is better suited for this particular dataset, likely due to its improved ability to handle outliers and avoid overfitting. 
To evaluate these statements, we plotted a regression line with the training data predictions for `livingSpace`, one of the variables with the highest correlation to `baseRent`. The plot indicates that the model demonstrates resilience to certain outliers and appears to generalize reasonably well, as its prediction is between the range of the minimum and maximum `baseRent`.

```{r regressionLineSVMLivingSpace, echo=FALSE, message=FALSE, warning=FALSE, cache=!full_refit, fig.align='center', fig.cap="Living Space vs Base Rent", fig.width=6, fig.height=4}
svm_model1 <- readRDS("data/models/models_svm.rds")[1]
train_pred_svm <- predict(svm_model1, train_data1)[[1]]

ggplot(train_data1, aes(x = livingSpace, y = baseRent)) +
  geom_point(color = 'blue', alpha = 0.5) + 
  geom_line(aes(x = livingSpace, y = train_pred_svm), color = 'red', size = 0.2) +
  labs(title = "Regression Line: Living Space vs Base Rent",
       x = "Living Space",
       y = "Base Rent") +
  theme_minimal()
```

## Random Forest
In this section, we focus on training a **random forest** using the `randomForest` package.
Random forests in R cannot natively handle variables with more than 53 categorical features. This restriction posed a challenge in our dataset, as the regio3 variable contains more than 53 unique categories. Consequently, we excluded this variable from the random forest training process. While this exclusion makes a direct comparison with other models somewhat unfair, we believe that random forests' ability to reduce noise and capture complex interactions still allows them to outperform single decision trees and SVMs in general.

To optimize the random forest model, we performed cross-validation on one of the imputed datasets using the following parameters, where the best parameter combination is indicated in bold.  

- **Number of Trees**: (100, **500**, 1000)  
   The number of trees in the forest. A higher number of trees usually improves performance but increases computation time.  

- **Maximum Features**: (2, 5, 10, **20**)  
  The number of features randomly selected at each split. A smaller value can help reduce overfitting (a common rule of thumb is to use \( \sqrt{n} \)), while a larger value allows the model to capture more complex relationships. 

- **Node Size**: (**2**, 5, 10, 20)  
   The minimum number of observations in a terminal node. Larger values result in simpler trees and can reduces complexity.

The results of random forest fitting on all imputed datasets, excluding the `regio3` variable, are summarized below:  

| Method                    | Average MSE  | Average RMSE | Average MAE  |
|---------------------------|--------------|--------------|--------------|
| **Random Forest**         | 92,827.36    | 304.65       | 167.32       |

To visualize the performance of our first model using imputed data, we generated a plot that illustrates the impact of each variable on the model based on the `incNodePurity`. A higher `incNodePurity` value indicates a greater influence of the variable on the random forest model.

```{r randomForestVarImportance, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Wordcloud of Titles", fig.align='center', fig.width=5, fig.height=4}
img <- readPNG("./documents/images/randomForestVarImportance.png")
grid::grid.raster(img)
```

We can observe that variables with high correlations exert the most influence on the random forest. Based on these findings, we plan to reduce the models complexity in a later chapter and explore whether a smaller model, trained on less noisy data, could outperform models trained on the full dataset.

Random forests demonstrated superior performance in terms of error metrics compared to both SVMs and single decision trees. The ensemble approach reduced the overall noise in the data and captured complex interactions among variables more effectively. Although the exclusion of `regio3` may have slightly impacted its performance.  



## Gradient Boosting Machine

In this section, we focus on training a **gradient boosting machine (GBM)** using the `gbm` package in R. 
Unlike random forests, gradient boosting emphasizes optimization by minimizing a loss function iteratively, which often results in highly predictive models, but with a higher risk of overfitting.

To optimize the GBM model, we performed a grid search using cross-validation on one of the imputed datasets. The parameters explored and their implications are as follows:  

- **Number of Trees** (100, 200, 300, **500**)  
  The number of boosting iterations. More trees typically improve performance up to a point but increase computational cost and the risk of overfitting.  

- **Tree Depth** (1, 2, **3**)  
  The maximum depth of each tree. A smaller depth helps prevent overfitting and ensures the model captures simpler interactions, while deeper trees can model more complex relationships.  

- **Shrinkage** (**0.05**, 0.1)  
  Also known as the learning rate, this parameter controls how much each tree contributes to the final model. Smaller values slow down the learning process and can lead to more robust models.  

- **Minimum Observations per Node** (**10**, 20)  
  The minimum number of observations in terminal nodes. Higher values result in simpler trees and prevent overfitting by reducing model complexity.

Using the best combination (**bold**), we evaluated the GBMs performance across all imputed datasets, summarized as follows:  

| Method                | Average MSE  | Average RMSE | Average MAE  |
|-----------------------|--------------|--------------|--------------|
| **Gradient Boosting** | 94,612.16    | 307.51       | 177.78       |  

A partial dependence plot illustrates the relationship between a predictor variable and the target variable while holding other features constant. This visualization helps interpret the influence of individual predictors on the models predictions and provides insights into how a specific variable affects the target. The plot below highlights the relationship between `livingSpace` and `baseRent`.

```{r pdpGbm, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="Wordcloud of Titles", fig.align='center', fig.width=5, fig.height=4}
img <- readPNG("./documents/images/pdp_gbm.png")
grid::grid.raster(img)
```

The partial dependence plot for `livingSpace` demonstrates a good fit, considering the [high correlation](#exploring-correlations-between-variables) with the target variable. The absence of abrupt changes in the plot highlights the robustness.

Gradient boosting only outperformed single decision trees in all error metrics. Compared to random forests and SVMs, GBMs demonstrated slightly higher errors but offered the advantage of interpretable partial dependence plots. While random forests are more robust to noise due to their averaging mechanism, gradient boosting prioritizes correcting previous errors, which can lead to overfitting to noise. This tendency may explain why the model underperforms compared to random forests and SVMs when dealing with our data, which contains a high levels of noise.


## XGBoost
TODO


## Outlier Detection
TODO


## Smaller Model
TODO
- mention random forest
- mention correlation
- mention boxplots
- mention that one could do PCA to see variables with high variance
- mention that one could do MCA to see categorical variables with high variance


# Model Evaluation
TODO
- plot predicted values of best model against real values
- plot distribution of predicted and trues
- include district evaluation 
- write something like a small conclusion
- mention that models could have performed different if we used less columns for all but we only done this for the best model


# Evaluation on current apartments
Why it might fail?
Market Demand and Supply: An imbalance between the number of available apartments and the demand from potential tenants leads to price fluctuations. High demand with limited supply results in increased rents. 
Economic Conditions: The general economic situation and income development of the population in Berlin play a role in determining rental prices. 
(https://eichenglobal.com/en/blog/mietspiegel-2024-berlin/)
Government Policies: Regulations such as rent caps or tenant protection laws can influence rental prices by limiting increases and affecting market dynamics. (https://www.pwc.de/en/real-estate/rent-cap-and-regulation-of-the-real-estate-market.html)

# References

::: {#refs}
:::
