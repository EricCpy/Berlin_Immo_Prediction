---
title: "ML2 Project - Prediction of rent cost in Berlin"
author: "Caroline Graebel"
format: html
---

# Load data and libraries
```{r}
#| include: false
library(tidyverse)
library(gbm)
library(e1071)
library(mice)
library(VIM)
library(caret)
library(here)
library(see)

immo_df <- readRDS(here("data/subset_by_district_immo.rds"))
```

# Random Forest Classifier

## Data Preparation
All variables that aren't useful for the model are removed. This is for 
saving calculation time for the models. We also have to impute values,
so the less columns the better to save runtime and thinking time over variables with
little information.
```{r}
str(immo_df)
```
The variable regio1 only holds "Berlin" for every row. This isn't useful. We take
it as context, that the data ist from Berlin only. <br>
scoutID also isn't useful under the assumption that each flat in this dataset is
unique. Is that the case?
```{r}
sort(table(immo_df$scoutId), decreasing = TRUE)[1:3]
```
When sorting the table by decreasing frequency, the first 3 results have a frequency
of 1. Therefore, each ID is unique. This means that the ID can be removed when
creating a table that only contains predictors and the label. <br>
Furthermore, we are only interested in variables that the "raw" information.
Because of that, variables that gives us a binned aggregation of a variable
aren't interesting. These variables all contain the word
"range".
```{r}
immo_df |>
  select(matches("Range")) |>
  names()
```
The base price for electricity is the same over all rows. So it doesn't give us
much distinction. It's either 90.76â‚¬ or missing. So it gets removed.
```{r}
sort(table(immo_df$electricityBasePrice), decreasing = TRUE)
```
This also applies for base price per kwh.
```{r}
sort(table(immo_df$electricityKwhPrice), decreasing = TRUE)
```
Since there are only 4 different dates, having the full date isn't necessary for
predicting. We can just treat the scraping dates as categories. So date_full gets
removed.
```{r}
table(immo_df$date)
```
Since we don't want to process text data, description and facilities also get
removed. <br>
The variable telekomHybridOffer only contains 10 or NA for hybrid upload speed. This
isn't great information, so it gets removed. <br>
The streetname gets also removed as it can't serve as a factor and therefore
isn't useful for a tree model. There are too many different streets.
```{r}
# select needed variables
immo_mod_df <- immo_df |>
  select(serviceCharge, heatingType, telekomTvOffer, newlyConst, pricetrend,
         balcony, telekomUploadSpeed, totalRent, yearConstructed, noParkSpaces,
         firingTypes, hasKitchen, cellar, baseRent, livingSpace, condition,
         interiorQual, petsAllowed, lift, typeOfFlat, noRooms,
         thermalChar, floor, numberOfFloors, garden, regio3,
         heatingCosts, energyEfficiencyClass, lastRefurbish, date)|>
  mutate_if(is.character, as.factor)

# sort so that label is first column
immo_mod_df <- immo_mod_df |>
  relocate(totalRent, .before = 1)
```
During selection, all remaining character type variables are transformed to factor
as they represent one of only a few categories. With this, our imputation method
also works for character type variables. The value imputation follows next.

## Value imputation
```{r}
#| include: false
#gibbs_immo <- mice(immo_mod_df, m = 5, maxit = 50, meth = 'pmm', seed = 600)

#saveRDS(gibbs_immo, here("data/gibbs_immo.rds"))

gibbs_immo <- readRDS(here("data/gibbs_immo.rds"))

# take first generated dataset
Gibbsimmo1 <- complete(gibbs_immo, 1)
```
To keep the distribution of variables and loose as little information as possible
when fitting our models, Gibbs sampling is used to impute values through a modelling
approach. With keeping missing values, we run into the trouble of needing to handling
them with fitting our models. Especially for a tree classifier, missing values
in testing data might lead to no result, as information is missing on what node
to take next. The resulting object of the mice algorithm holds eight different
datasets that might be more or less different from one another. For starting off,
the first dataset is chosen.

# Gradient Boosting Machine (GBM)

## Train and Test Split
```{r}
set.seed(10)
train_share <- 0.7
train_idx <- sample(1:nrow(Gibbsimmo1), size = as.integer(nrow(Gibbsimmo1) * train_share))
Immotrain <- Gibbsimmo1[train_idx, ]
Immotest <- Gibbsimmo1[-train_idx, ]
```

## Modelling
To fit a solid model, we use cross-validated grid search over a given grid of parameters
to find the best performing one. By using cross-validation, we include a validation step
in each loop, so that the model isn't only evaluated on the training data.
```{r}
#| include: false
hyper_grid <- expand.grid(
  interaction.depth = c(1, 2, 3),
  n.trees = c(100, 200, 500),
  shrinkage = c(0.01, 0.1, 0.2),
  n.minobsinnode = c(5, 10, 20)
)

# create a training control object with cross-validation
train_control <- trainControl(method = "cv", number = 5)

# train the model with cross-validation and grid search
model <- train(
  totalRent ~ .,
  data = Immotrain,
  method = "gbm",
  trControl = train_control,
  tuneGrid = hyper_grid
)

#saveRDS(model, here("data/gbm_model.rds"))
```
Let's look at the resulting model and the best parameters.
```{r}
model
```

## Model performance
Since our label is a continuous variable, the root mean squared error can be used
to compare the predicted values with the true values.
```{r}
predictions <- predict(model, newdata = Immotest)
rmse <- RMSE(predictions, Immotest$totalRent)
print(rmse)
```
```{r}
plot(model)
```



