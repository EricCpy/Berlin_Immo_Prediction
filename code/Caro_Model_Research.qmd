---
title: "ML2 Project - Prediction of rent cost in Berlin"
author: "Caroline Graebel"
format: html
---

# Load data and libraries
```{r}
library(tidyverse)
library(gbm)
library(e1071)
library(mice)
library(VIM)
library(caret)
library(here)

immo_df <- readRDS(here("data/subset_by_district_immo.rds"))
```

# Random Forest Classifier

## Data Preparation
All variables that aren't useful for the model are removed. This is for 
saving calculation time for the models. We also have to impute values,
so the less columns the better to save runtime and thinking time over variables with
little information.
```{r}
str(immo_df)
```
The variable regio1 only holds "Berlin" for every row. This isn't useful. We take
it as context, that the data ist from Berlin only. <br>
scoutID also isn't useful under the assumption that each flat in this dataset is
unique. Is that the case?
```{r}
sort(table(immo_df$scoutId), decreasing = TRUE)[1:3]
```
When sorting the table by decreasing frequency, the first 3 results have a frequency
of 1. Therefore, each ID is unique. This means that the ID can be removed when
creating a table that only contains predictors and the label. <br>
Furthermore, we are only interested in variables that the "raw" information.
Because of that, variables that gives us a binned aggregation of a variable
aren't interesting. These variables all contain the word
"range".
```{r}
immo_df |>
  select(matches("Range")) |>
  names()
```
The base price for electricity is the same over all rows. So it doesn't give us
much distinction. It's either 90.76â‚¬ or missing. So it gets removed.
```{r}
sort(table(immo_df$electricityBasePrice), decreasing = TRUE)
```
This also applies for base price per kwh.
```{r}
sort(table(immo_df$electricityKwhPrice), decreasing = TRUE)
```
Since there are only 4 different dates, having the full date isn't necessary for
predicting. We can just treat the scraping dates as categories. So date_full gets
removed.
```{r}
table(immo_df$date)
```
Since we don't want to process text data, description and facilities also get
removed. <br>
Lastly, telekomHybridOffer only contains 10 or NA for hybrid upload speed. This
isn't great information, so it gets removed.
```{r}
# select needed variables
immo_mod_df <- immo_df |>
  select(serviceCharge, heatingType, telekomTvOffer, newlyConst, pricetrend,
         balcony, telekomUploadSpeed, totalRent, yearConstructed, noParkSpaces,
         firingTypes, hasKitchen, cellar, baseRent, livingSpace, condition,
         interiorQual, petsAllowed, streetPlain, lift, typeOfFlat, noRooms,
         thermalChar, floor, numberOfFloors, garden, regio3, description,
         heatingCosts, energyEfficiencyClass, lastRefurbish, date)

# sort so that label is first column
immo_mod_df <- immo_mod_df |>
  relocate(totalRent, .before = 1)
```

## Value imputation
```{r}
#| include: false
#gibbs_immo <- mice(immo_mod_df, m = 5, maxit = 50, meth = 'pmm', seed = 600)

#saveRDS(gibbs_immo, here("data/gibbs_immo.rds"))

gibbs_immo <- readRDS(here("data/gibbs_immo.rds"))

# take first generated dataset
Gibbsimmo1 <- complete(gibbs_immo, 1)
```

# Gradient Boosting Machine (GBM)

## Train and Test Split
```{r}
set.seed(10)
train_share <- 0.7
train_idx <- sample(1:nrow(Gibbsimmo1), size = as.integer(nrow(Gibbsimmo1) * train_share))
Immotrain <- Gibbsimmo1[train_idx, ]
Immotest <- Gibbsimmo1[-train_idx, ]
```

## Modelling
```{r}
hyper_grid <- expand.grid(
  interaction.depth = c(1, 2, 3),
  n.trees = c(100, 200, 500),
  shrinkage = c(0.01, 0.1, 0.2)
)

# Create a training control object with cross-validation
train_control <- trainControl(method = "cv", number = 5)

# Train the model with cross-validation and grid search
model <- train(
  totalRent ~ .,
  data = Immotrain,
  method = "gbm",
  trControl = train_control,
  tuneGrid = hyper_grid,
  na.action = na.omit
)
```


